# oss 강화학습

## day 1 

* what is the 강화학습?  
    스키너의 강화 연구  
        굶긴 쥐를 상자에 넣고, 버튼을 누르면, 먹이가 나오게 하는데,  
        쥐는 돌아다니다가 실수로 버튼을 누르게 되고,   
        그 버튼을 누르면 먹이가 나온다는 것을 알게되고, 이것을 학습한다.  
    
    강화학습
        에이전트는 사진 지식이 없는 상태에서 학습한다.  
        에이전트는 자신이 놓인 환경에서 자신의 상태를 인식한 후에 행동한다.  
        환경은 에이전트에게 보상을 주고 다음 상태를 알려준다.     
        에이전트는 보상을 통해, 어떤 행동이 좋은 행동인지 간접적으로 알게된다.

    강화학습 문제
        결정을 순차적으로 내려야 하는 문제에 강화학습을 적용 가능하다.  
        이 문제를 풀기 위해서는 문제를 수학적으로 정의해야 한다.    

        수학적으로 정의된 문제는 다음과 같은 구성요소를 가짐
        1. state
            현재 에이전트의 정보(정적인 요소 + 동적인 요소)
        2. action
            에이전트가 어떠한 상태에서 취할 수 있는 행동
        3.reward
            에이전트가 유일하게 학습 할 수 있는 정보,자신이 했던 행동을 평가 할 수 있는 지표.  
            강화학습의 목표는 시간에 따라 얻는 보상의 합을 최대로 하는 방법을 찾는 것이 목표이다.  
        4.policy
            순차적 행동 결정 문제에서 구해야 할 답.
* MDP  
    (markov decision process)
    MDP의 구성 요소
    상태,행동,보상함수,상태 변환 확률,감가율,정책

    상태
        에이전트가 관찰 가능한 상태의 집합 : S  
        그리드 월드를 예시로 들면 빈칸 하나하나가 상태가 된다.  
        상태는 항상 바뀔 수 있기 때문에, 상태는 확률변수가 된다.(확률에 따라 값이 변하는 변수)  
        St = s  
        시간 t에서의 상태 St가 어떤 상태 s이다.    

    행동    
        에이전트가 St 에서 할 수 있는 가능한 행동의 집합.  
        At = a 시간 t에서 에이전트가 특정한 a를 했다.  

    보상함수  
        시간 t일 때, 상태가 St = s이고 그 상태에서 행동이 At = a를 했을 경우,  
        받을 보상에 대한 기댓값이 E  
        에이전트가 어떤 상태에서 행동한 시간 t
        보상을 받는 시간 t+1  
         
        이유 : 에이전트가 보상을 알고 있는게 아니고, 환경이 알려주기 때문에 시간이 지나야 보상을 받는다.

    상태 변환 확률
        에이전트가 어떤 상태에서 어떤 행동을 취하면, 상태가 변한다.  
        하지만, 어떤 이유로 인해 다음 상태로 변하지 못할 수도 있다.  
        상태 변화에도 확률적인  요인이 들어간다.    
        이를 수치적으로 표현한 것이 상태 변환 확률   
    
    감가율  
        에이전트는 항상 현재 시점에서 판단을 내리기 때문에 현재에 가까운 보상일수록 더 큰 가치를 갖는다.  
        같은 보상을 받을경우, 나중에 보상을 받을수록, 가치가 줄어든다.  
        현재 시간 t로부터 시간 k가 지난 후에 받는 보상이 R t+k 라면, 현재 그 보상의 가치는 Y k-1 R t+k와 같다.  

    정책
        모든 상태에서 에이전트가 할 행동
        파이(a|s) = P[At = a|St = s]


* 가치함수  
    * 가치함수
        에이전트 입장에서 어떤 행동을 하는 것이 좋은지를 어떻게 알 수 있을까?  
        --> 현재 상태에서 앞으로 받을 보상을 고려하여 선택해야 좋은 선택을 할 수 있다.

        현재 시간 t부터 에이전트가 행동을 하면서 받을 보상을 더한 수식은 문제가 있다.    
        감가율 문제 --> 시간 개념이 더해지지 않았다. 등등 여러개 있다.  

        따라서 보상의 합으로는 판단을 내리기 힘들기 때문에, 정확한 판단을 위해 감가율을 고려한다.  
        이 값을 반환값이라고 한다.  

        에이전트는 에피소드가 끝난 후에만 반환값을 알 수 있다.    
        그러면 에피소드가 끝날 때까지 기다려야 하는 것인가?  
        정확하지 않더라도 현재의 정보를 토대로 행동하는 것이 나을 때가 있다.  

        가치함수 = 어떠한 상태에 가면 받을 것이라고 예상되는 값  
        v(s) = E[G t | S t = s]    

        하지만 정책을 고려해야 한다.  
        정책에 따라서 가치함수는 달라질 수 밖에 없다.  

        벨만 기대 방정식  
        현재 가치함수와 다음 가치함수의 사이의 관계를 의미한다.  

    * 큐함수
        현재 상태를 넣고 가치함수를 돌리고 받을 보상의 합이 나오는게 큐 함수  
        어떠한 상태에서 어떤 행동이 좋은지 알려주는 함수를 행동 가치함수 라고 한다.  
        이렇게 되면 위의 가치함수에서 다음 행동의 value값을 알아야 하는 것을 안해도 된다.    
        큐 함수는 가치함수를 행동 단위로 쪼개 놓은 것이라고 하면 될듯 하다.  


* 다이나믹 프로그래밍
    한번에 풀기 어려운 문제를 여러 개의 작은 문제로 나눠 푸는 것
    

* 정책 평가

* 알고리즘